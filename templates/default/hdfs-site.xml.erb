<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

  <property>
    <name>dfs.replication</name>
    <value><%= node['hops']['num_replicas'] %></value>
    <description>Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    </description>
  </property>

  <property>
    <name>dfs.namenode.rpc-address</name>
    <value><%= @firstNN %></value>
    <description>
      RPC address that handles all clients requests. In the case of HA/Federation where multiple namenodes exist,
      the name service id is added to the name e.g. dfs.namenode.rpc-address.ns1
      dfs.namenode.rpc-address.EXAMPLENAMESERVICE
      The value of this property will take the form of hdfs://nn-host1:rpc-port.
    </description>
  </property>


  <property>
    <name>dfs.client.max.retries.on.failure</name>
    <value>1</value>
  </property>

  <property>
    <name>dfs.client.block.write.locateFollowingBlock.retries</name>
    <value>10</value>
  </property>

  <!-- Do not modify this file directly.  Instead, copy entries that you -->
  <!-- wish to modify from this file into hdfs-site.xml and change them -->
  <!-- there.  If hdfs-site.xml does not already exist, create it.      -->

  <!-- <property> -->
  <!--   <name>hadoop.hdfs.configuration.version</name> -->
  <!--   <value>1</value> -->
  <!--   <description>version of this configuration file</description> -->
  <!-- </property> -->

  <property>
    <name>dfs.namenode.accesstime.precision</name>
    <value>3600000</value>
    <description>The access time for HDFS file is precise upto this value. 
    The default value is 1 hour. Setting a value of 0 disables
    access times for HDFS.
    </description>
  </property>


  <property>
    <name>dfs.namenode.handler.count</name>
    <value>120</value>
    <description>The RPC server that listens to requests from clients</description>
  </property>

  <!-- <property> -->
  <!--    <name>dfs.namenode.service.handler.count</name> -->
  <!--   <value>10</value> -->
  <!--   <description>The RPC server threads that listens to requests from DataNodes</description> -->
  <!-- </property> -->

  <!-- <property> -->
  <!--   <name>dfs.datanode.data.dir</name> -->
  <!--   <value>file://<%= node['hops']['dn']['data_dir'] %></value> -->
  <!--   <description>Determines where on the local filesystem an DFS data node should store its blocks.  If this is a comma-delimited -->
  <!--   list of directories, then data will be storned in all named  directories, typically on different devices. Directories that do not exist are ignored. -->
  <!--   </description> -->
  <!-- </property> -->

  <property>
    <name>dfs.namenode.inodeid.batchsize</name>
    <value>100000</value>
    <description></description>
  </property>

  <property>
    <name>dfs.namenode.blockid.batchsize</name>
    <value>100000</value>
    <description></description>
  </property>

  <property>
    <name>dfs.blocksize</name>
    <value><%= node['hops']['hdfs']['blocksize'] %></value>
    <description>
        The default block size for new files, in bytes.
        You can use the following suffix (case insensitive):
        k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),
        Or provide complete size in bytes (such as 134217728 for 128 MB).
    </description>
  </property>

  <property>
    <name>dfs.client.refresh.namenode.list</name>
    <value>60000</value>
    <description>Time in ms</description>
  </property>


  <property>
    <name>dfs.namenode.name.dir</name>
    <value><%= node['hops']['nn']['name_dir'] %></value>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value><%= node['hops']['dn']['data_dir'] %></value>
  </property>

  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value><%= node['hops']['dn']['data_dir_permissions'] %></value>
  </property>  

  <property>
    <name>dfs.namenode.safemode.extension</name>
    <value>30000</value>
    <description>
      Determines extension of safe mode in milliseconds
      after the threshold level is reached.
    </description>
  </property>

  <property>
    <name>dfs.namenode.processReport.batchsize</name>
    <value>10000</value>
    <description>This is the number of blocks to be processed in one transaction.
    Higher values can result in higher throughput, but too high values can cause transactions to fail.
    </description>
  </property>

  <property>
    <name>dfs.namenode.misreplicated.batchsize</name>
    <value>500</value>
    <description>
      
    </description>
  </property>

  <property>
    <name>dfs.namenode.misreplicated.noofbatches</name>
    <value>10</value>
    <description>
    </description>
  </property>

  <!-- <property> -->
  <!--   <name>dfs.namenode.logging.level</name> -->
  <!--   <value><%= node['hops']['log_level'] %></value> -->
  <!--   <description> -->
  <!--     The logging level for dfs namenode. Other values are "dir" (trace -->
  <!--     namespace mutations), "block" (trace block under/over replications -->
  <!--     and block creations/deletions), or "all". -->
  <!--   </description> -->
  <!-- </property> -->


  <property>
    <name>dfs.namenode.selector-policy</name>
    <value>RANDOM_STICKY</value>
    <description>Used by clients. Possible values ROUND_ROBIN, RANDOM, RANDOM_STICKY</description>
  </property>


  <property>
    <name>dfs.resolvingcache.memcache.connectionpool.size</name>
    <value>10</value>
    <description>should be same size as rpc threaads</description>
  </property>

  <property>
    <name>dfs.resolvingcache.enabled</name>
    <value><%= node['hops']['nn']['cache'] %></value>
    <description></description>
  </property>

  <property>
    <name>dfs.resolvingcache.type</name>
    <value>InMemory</value>
    <description></description>
  </property>

  <property>
    <name>dfs.resolvingcache.inmemory.maxsize</name>
    <value>2000000</value>
    <description></description>
  </property>

  <property>
    <name>dfs.ndb.setpartitionkey.enabled</name>
    <value><%= node['hops']['nn']['partition_key'] %></value>
    <description></description>
  </property>

  <property>
    <name>dfs.ndb.setrandompartitionkey.enabled</name>
    <value>true</value>
    <description></description>
  </property>

  <property>
    <name>dfs.namenode.quota.enabled</name>
    <value>true</value>
    <description></description>
  </property>

  <property>
    <name>dfs.permissions.enabled</name>
    <value>true</value>
    <description></description>
  </property>

  <property>
    <name>fs.permissions.umask-mode</name>
    <value><%= node['hops']['hdfs']['umask'] %></value>
    <description></description>
  </property>
  

  <property>
    <name>dfs.ndc.enable</name>
    <value>false</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.enabled</name>
    <value>false</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.detailed.enabled</name>
    <value>false</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.writerround</name>
    <value>30</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.dir</name>
    <value>/tmp/hopsstats</value>
    <description></description>
  </property>

  <property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
    <description></description>
  </property>

  <!-- Enable erasure coding to test all code paths -->

  <property>
    <name>dfs.erasure_coding.enabled</name>
    <value><%= node['hops']['erasure_coding'] %></value>    
    <description>Enable erasure coding</description>
  </property>

  <property>
     <name>dfs.webhdfs.enabled</name>
     <value>true</value>
  </property>


  <property>
    <name>dfs.permissions.superusergroup</name>
    <value><%= node['hops']['hdfs']['user'] %></value>    
    <description>The group for hdfs superusers</description>
  </property>


<!--
  <property>
    <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
    <value><%= node['hops']['reverse_dns_lookup_supported'] %></value>
  </property>

  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value><%= node['hops']['tmp_dir'] %>/dn_socket</value>
  </property>

-->

<!-- SSL -->
<!-- Secure webhdfs by requiring TLS two-way authentication, port
     50070 should be firewalled -->
<% if node['hops']['tls']['enabled'].eql? "true" -%>
<property>
  <name>dfs.https.enable</name>
  <value><%= node['hops']['dfs']['https']['enable'] %></value>
</property>

<property>
  <name>dfs.http.policy</name>
  <value><%= node['hops']['dfs']['http']['policy'] %></value>
</property>

<property>
  <name>dfs.https.port</name>
  <value><%= node['hops']['dfs']['https']['port'] %></value>
</property>

<property>
  <name>dfs.datanode.https.address</name>
  <value><%= node['hops']['dfs']['datanode']['https']['address'] %></value>
</property>

<property>
  <name>dfs.namenode.http-address</name>
  <value><%= @nnHTTPAddress %></value>
</property>

<property>
  <name>dfs.client.https.need-auth</name>
  <value>true</value>
</property>
<% end %>

<!-- Store Small Files in NDB-->

<property>
   <name>dfs.store.small.files.in.db</name>
   <value><%= node['hops']['small_files']['store_in_db'] %></value>
</property>

<property>
   <name>dfs.db.file.max.size</name>
   <value><%= node['hops']['small_files']['max_size'] %></value>
</property>

<property>
   <name>dfs.db.ondisk.small.file.max.size</name>
   <value><%= node['hops']['small_files']['on_disk']['max_size']['small']  %></value>
</property>

<property>
   <name>dfs.db.ondisk.medium.file.max.size</name>
   <value><%= node['hops']['small_files']['on_disk']['max_size']['medium']  %></value>
</property>

<property>
   <name>dfs.db.ondisk.large.file.max.size</name>
   <value><%= node['hops']['small_files']['on_disk']['max_size']['large'] %></value>
</property>

<property>
   <name>dfs.db.inmemory.file.max.size</name>
   <value><%= node['hops']['small_files']['in_memory']['max_size'] %></value>
</property>

</configuration>
